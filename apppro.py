import streamlit as st
import joblib
import numpy as np
import pandas as pd
import os
import re
import torch
from transformers import BertTokenizer, BertModel
from collections import Counter
import warnings

warnings.filterwarnings('ignore')


# ===============================
# 1. åŠ è½½æ¨¡å‹ä¸èµ„æº
# ===============================
@st.cache_resource
def load_resources():
    base = "F:/å·¥ä½œ/ä¸Šå¤§/NLP/test2/saved_models"

    # åŠ è½½é›†æˆ BERT çš„æ¨¡å‹
    model = joblib.load(os.path.join(base, "ensemble_with_bert.pkl"))
    vectorizer = joblib.load(os.path.join(base, "tfidf_vectorizer.pkl"))
    scaler = joblib.load(os.path.join(base, "scaler_with_bert.pkl"))
    config = joblib.load(os.path.join(base, "feature_config_with_bert.pkl"))

    # åŠ è½½ BERT æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert_model = BertModel.from_pretrained('bert-base-uncased')
    bert_model.to(device)
    bert_model.eval()

    # åŠ è½½ä»‡æ¨è¯è¯å…¸
    hate_words_set = set()
    try:
        # ä»è®­ç»ƒæ—¶ä¿å­˜çš„ sensitive words
        sensitive_words = joblib.load(os.path.join(base, "sensitive_words.pkl"))
        hate_words_set = set(w.lower() for w in sensitive_words)
    except:
        pass

    # ä» hatebase è¯å…¸åŠ è½½
    try:
        hate_dict_path = "F:/å·¥ä½œ/ä¸Šå¤§/NLP/test2/dictionaries/hatebase_dict.csv"
        hate_df = pd.read_csv(hate_dict_path, encoding='ISO-8859-1', header=None)
        for word in hate_df[0]:
            if isinstance(word, str):
                hate_words_set.add(word.strip("', ").lower())
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åŠ è½½ hatebase è¯å…¸: {e}")

    # æ·»åŠ å¸¸è§ä»‡æ¨è¯ä½œä¸ºåå¤‡
    common_hate_words = [
        'nigger', 'nigga', 'faggot', 'fag', 'retard', 'cunt',
        'bitch', 'slut', 'whore', 'bastard', 'damn', 'shit',
        'fuck', 'ass', 'piss', 'pussy', 'cock','dick'
    ]
    hate_words_set.update(common_hate_words)

    return {
        "model": model,
        "vectorizer": vectorizer,
        "scaler": scaler,
        "config": config,
        "bert_tokenizer": bert_tokenizer,
        "bert_model": bert_model,
        "device": device,
        "sensitive_words": hate_words_set,
        "hate_words": hate_words_set,  # ç”¨äºå±è”½
        "feature_names": config['feature_names']
    }


resources = load_resources()


# ===============================
# 2. æ–‡æœ¬é¢„å¤„ç†
# ===============================
def preprocess(text):
    """ç®€å•çš„æ–‡æœ¬é¢„å¤„ç†"""
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#", "", text)
    return text


# ===============================
# 2.5. ä»‡æ¨è¯å±è”½åŠŸèƒ½ (æ–°å¢)
# ===============================
def mask_hate_words(text, hate_words_set):
    """
    å°†æ–‡æœ¬ä¸­çš„ä»‡æ¨è¯ç”¨ * å±è”½

    å‚æ•°:
        text: åŸå§‹æ–‡æœ¬
        hate_words_set: ä»‡æ¨è¯é›†åˆ

    è¿”å›:
        masked_text: å±è”½åçš„æ–‡æœ¬
        found_words: æ‰¾åˆ°çš„ä»‡æ¨è¯åˆ—è¡¨
    """
    words = text.split()
    masked_words = []
    found_words = []

    for word in words:
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·è¿›è¡ŒåŒ¹é…
        clean_word = re.sub(r'[^\w\s]', '', word.lower())

        if clean_word in hate_words_set:
            # ä¿ç•™é¦–å°¾å­—ç¬¦,ä¸­é—´ç”¨*æ›¿æ¢
            if len(clean_word) <= 2:
                masked = '*' * len(clean_word)
            else:
                masked = clean_word[0] + '*' * (len(clean_word) - 2) + clean_word[-1]

            # æ¢å¤åŸå§‹çš„æ ‡ç‚¹ç¬¦å·
            masked_word = word.lower().replace(clean_word, masked)
            masked_words.append(masked_word)
            found_words.append(clean_word)
        else:
            masked_words.append(word)

    return ' '.join(masked_words), found_words


# ===============================
# 3. æå– BERT ç‰¹å¾
# ===============================
def extract_bert_features(text, r):
    """æå–å•æ¡æ–‡æœ¬çš„ BERT ç‰¹å¾"""
    encoded = r['bert_tokenizer'](
        text,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

    input_ids = encoded['input_ids'].to(r['device'])
    attention_mask = encoded['attention_mask'].to(r['device'])

    with torch.no_grad():
        outputs = r['bert_model'](input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()

    return cls_embedding.flatten()


# ===============================
# 4. ç‰¹å¾æå– (ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´)
# ===============================
def extract_features(text, r):
    """
    æå–ç‰¹å¾,å¿…é¡»ä¸è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºå’Œæ•°é‡å®Œå…¨ä¸€è‡´
    """
    feature_names = r['feature_names']
    features = {}

    # 1. Weighted TF-IDF Score
    if any('weighted_TFIDF_scores' in name for name in feature_names):
        words = text.lower().split()
        score = sum(1 for w in words if w in r['sensitive_words'])
        features['weighted_TFIDF_scores'] = score

    # 2. Sentiment Features
    sentiment_cols = ['hate', 'hatenor', 'neg', 'negnor', 'pos', 'posnor']
    if any(col in feature_names for col in sentiment_cols):
        words = text.lower().split()

        # ç®€åŒ–çš„æƒ…æ„Ÿç‰¹å¾è®¡ç®—
        hate_count = sum(1 for w in words if w in r['sensitive_words'])
        hate_ratio = hate_count / len(words) if len(words) > 0 else 0

        # ç®€å•çš„æ­£è´Ÿé¢è¯æ£€æµ‹
        positive_words = ['good', 'great', 'awesome', 'excellent', 'love', 'like']
        negative_words = ['bad', 'terrible', 'awful', 'hate', 'stupid', 'dumb']

        pos_count = sum(1 for w in words if w in positive_words)
        neg_count = sum(1 for w in words if w in negative_words)

        pos_ratio = pos_count / len(words) if len(words) > 0 else 0
        neg_ratio = neg_count / len(words) if len(words) > 0 else 0

        if 'hate' in feature_names:
            features['hate'] = hate_count
        if 'hatenor' in feature_names:
            features['hatenor'] = hate_ratio
        if 'neg' in feature_names:
            features['neg'] = neg_count
        if 'negnor' in feature_names:
            features['negnor'] = neg_ratio
        if 'pos' in feature_names:
            features['pos'] = pos_count
        if 'posnor' in feature_names:
            features['posnor'] = pos_ratio

    # 3. Dependency Features (å ä½)
    dep_features = [name for name in feature_names if name.startswith('dep_')]
    for dep_name in dep_features:
        features[dep_name] = 0

    # 4. TF-IDF Features
    tfidf_matrix = r['vectorizer'].transform([text]).toarray()[0]
    for i, val in enumerate(tfidf_matrix):
        col_name = f'tfidf:{i}'
        if col_name in feature_names:
            features[col_name] = val

    # 5. BERT Features (æ ¸å¿ƒæ–°å¢éƒ¨åˆ†)
    if r['config']['used_bert']:
        bert_features = extract_bert_features(text, r)
        for i, val in enumerate(bert_features):
            col_name = f'bert:{i}'
            if col_name in feature_names:
                features[col_name] = val

    # æŒ‰ç…§è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºæ„å»ºç‰¹å¾å‘é‡
    feature_vector = []
    for name in feature_names:
        feature_vector.append(features.get(name, 0))

    X = np.array(feature_vector).reshape(1, -1)

    # æ ‡å‡†åŒ–
    X_scaled = r['scaler'].transform(X)
    return X_scaled


# ===============================
# 5. Streamlit UI
# ===============================
st.set_page_config(page_title="Hate Speech Detection", layout="centered")
st.title("ğŸ›¡ï¸ Hate Speech Detection")
st.caption("åŸºäºæœºå™¨å­¦ä¹ æ¨¡å‹çš„ä»‡æ¨è¨€è®ºæ£€æµ‹ç³»ç»Ÿ")

# æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
with st.expander("â„¹ï¸ æ¨¡å‹ä¿¡æ¯"):
    st.write(f"ç‰¹å¾ç»´åº¦: {resources['config']['feature_dim']}")
    st.write(f"ä½¿ç”¨çš„ç‰¹å¾ç±»å‹:")
    st.write(f"- Weighted TF-IDF: {resources['config']['used_weighted_tfidf']}")
    st.write(f"- Sentiment: {resources['config']['used_sentiment']}")
    st.write(f"- Dependency: {resources['config']['used_dependency']}")
    st.write(f"- TF-IDF: {resources['config']['used_tfidf']}")
    st.write(f"- BERT: {resources['config']['used_bert']} ")
    st.write(f"- ä»‡æ¨è¯è¯å…¸å¤§å°: {len(resources['hate_words'])} ä¸ªè¯")

text = st.text_area("è¯·è¾“å…¥è‹±æ–‡æ–‡æœ¬", height=150, placeholder="Enter English text here...")

if st.button("ğŸ” æ£€æµ‹"):
    if not text.strip():
        st.warning("è¯·è¾“å…¥æ–‡æœ¬")
    else:
        with st.spinner("æ¨¡å‹æ¨ç†ä¸­..."):
            try:
                X = extract_features(text, resources)

                # é¢„æµ‹
                try:
                    proba = resources["model"].predict_proba(X)[0]
                    pred = np.argmax(proba)
                except AttributeError:
                    pred = resources["model"].predict(X)[0]
                    proba = np.zeros(3)
                    proba[pred] = 1.0

                # æ˜¾ç¤ºç»“æœ
                labels_cn = ["ä»‡æ¨è¨€è®º", "æ”»å‡»æ€§è¯­è¨€", "æ­£å¸¸è¨€è®º"]
                labels_en = ["Hate Speech", "Offensive Language", "Neither"]

                st.subheader("ğŸ“Š åˆ†ç±»ç»“æœ")

                # æ ¹æ®é¢„æµ‹ç»“æœæ˜¾ç¤ºä¸åŒé¢œè‰²
                if pred == 0:
                    st.error(f"**{labels_cn[pred]} ({labels_en[pred]})**")
                elif pred == 1:
                    st.warning(f"**{labels_cn[pred]} ({labels_en[pred]})**")
                else:
                    st.success(f"**{labels_cn[pred]} ({labels_en[pred]})**")

                st.subheader("ğŸ“ˆ ç½®ä¿¡åº¦")
                for i, p in enumerate(proba):
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.write(f"{labels_cn[i]} ({labels_en[i]})")
                    with col2:
                        st.write(f"{p:.2%}")
                    st.progress(float(p))

                # ===== æ–°å¢: ä»‡æ¨è¯å±è”½åŠŸèƒ½ =====
                if pred in [0, 1]:  # å¦‚æœæ£€æµ‹åˆ°ä»‡æ¨è¨€è®ºæˆ–æ”»å‡»æ€§è¯­è¨€
                    st.subheader("ğŸš« å±è”½åçš„æ–‡æœ¬")

                    masked_text, found_words = mask_hate_words(text, resources['hate_words'])

                    if found_words:
                        st.info(masked_text)
                        st.caption(f"æ£€æµ‹åˆ° {len(found_words)} ä¸ªæ•æ„Ÿè¯å¹¶å·²å±è”½")

                        # å¯é€‰: æ˜¾ç¤ºè¢«å±è”½çš„è¯(ç”¨äºè°ƒè¯•)
                        with st.expander("ğŸ” æŸ¥çœ‹è¢«å±è”½çš„è¯"):
                            st.write(", ".join(set(found_words)))
                    else:
                        st.info("æœªæ£€æµ‹åˆ°ä»‡æ¨è¯å…¸ä¸­çš„è¯æ±‡,ä½†æ¨¡å‹è¯†åˆ«ä¸ºä¸å½“å†…å®¹")
                        st.caption("å¯èƒ½åŒ…å«éšæ™¦è¡¨è¾¾æˆ–ä¸Šä¸‹æ–‡ä¸å½“")

            except Exception as e:
                st.error(f"æ£€æµ‹å¤±è´¥: {str(e)}")
                st.exception(e)

